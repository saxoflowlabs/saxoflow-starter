default_provider: groq
default_model: llama3-8b-8192
default_temperature: 0.3
default_max_tokens: 8192

agent_models:
  rtlgen:
    provider: groq
    model: llama3-8b-8192
    temperature: 0.3
    max_tokens: 8192
  tbgen:
    provider: mistral
    model: mistral-large-latest
    temperature: 0.3
    max_tokens: 8192
  fpropgen:
    provider: together
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    temperature: 0.3
    max_tokens: 8192
  rtlreview:
    provider: together
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    temperature: 0.3
    max_tokens: 8192
  tbreview:
    provider: groq
    model: llama3-8b-8192
    temperature: 0.3
    max_tokens: 8192
  fpropreview:
    provider: groq
    model: llama3-8b-8192
    temperature: 0.3
    max_tokens: 8192
  sim:
    provider: none
    model: none
  debug:
    provider: fireworks
    model: accounts/fireworks/models/llama4-maverick-instruct-basic
    temperature: 0.3
    max_tokens: 4999
  report:
    provider: mistral
    model: mistral-large-latest
    temperature: 0.3
    max_tokens: 8192

providers:
  groq:
    model: llama3-8b-8192
    temperature: 0.3
    max_tokens: 8192
  together:
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    temperature: 0.3
    max_tokens: 8192
  fireworks:
    model: accounts/fireworks/models/firellava-13b
    temperature: 0.3
    max_tokens: 4999
  openrouter:
    model: deepseek/deepseek-r1-0528-qwen3-8b:free
    temperature: 0.3
    max_tokens: 8192
  mistral:
    model: mistral-large-latest
    temperature: 0.3
    max_tokens: 8192
  googleaistudio:
    model: gemini-2.0-flash
    temperature: 0.3
    max_tokens: 8192
