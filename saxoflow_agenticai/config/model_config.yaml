# saxoflow_agenticai/config/model_config.yaml
default_provider: auto
default_model: auto
default_temperature: 0.3
default_max_tokens: 8192
timeout: 60
# enable_fallback_to_openrouter: true

autodetect_priority:
  - openai
  - anthropic      # <— new
  - gemini         # <— new
  - groq
  - mistral
  - fireworks
  - together
  - perplexity
  - deepseek
  - dashscope
  - openrouter

agent_models:
  sim:
    provider: none
    model: none

providers:
  openai:
    model: gpt-4o-mini
    temperature: 0.3
    max_tokens: 8192

  groq:
    model: llama3-8b-8192
    temperature: 0.3
    max_tokens: 8192

  mistral:
    model: mistral-large-latest
    temperature: 0.3
    max_tokens: null     # <— avoid 422 (extra field)

  fireworks:
    model: accounts/fireworks/models/llama-v3p1-8b-instruct
    temperature: 0.3
    max_tokens: 8192

  together:
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    temperature: 0.3
    max_tokens: 8192

  perplexity:
    model: llama-3.1-sonar-large-128k-online
    temperature: 0.3
    max_tokens: 8192

  deepseek:
    model: deepseek-chat
    temperature: 0.3
    max_tokens: 8192

  dashscope:
    model: qwen2.5-7b-instruct
    temperature: 0.3
    max_tokens: null     # <— conservative like mistral

  openrouter:
    model: anthropic/claude-3.5-sonnet
    temperature: 0.3
    max_tokens: 8192

  # New native providers
  anthropic:
    model: claude-3-5-sonnet-latest
    temperature: 0.3
    max_tokens: 4096     # Anthropic often expects a value; 4k is a safe middle ground

  gemini:
    model: gemini-1.5-pro
    temperature: 0.3
    max_tokens: null     # let SDK choose (maps to max_output_tokens)
